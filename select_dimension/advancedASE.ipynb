{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Limitation of Adjacency Spectral Embed and Extensions MASE and Omnibus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjacency Spectral Embedding (ASE) is a very useful way to reduce dimensionality while maintaining the essential information in a graph (for more details on how ASE works see the [tutorial](https://graspy.neurodata.io/tutorials/embedding/adjacencyspectralembed)).  However, in general, the ASE algorithm requires *a priori* knowledge of how many dimensions any given graph *truly* represents in order to faithfully embed it.  To satisfy this requirement, our ``AdjacencySpectralEmbed()`` class employs an algorithm to estimate the true number of dimensions $d$, by using a maximum profile likelihood approach applied to the singular values of the adjacency matrix.  However, this approach has some known limitations, one of which we will demonstrate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import graspy\n",
    "from graspy.simulations import sbm\n",
    "from graspy.embed import AdjacencySpectralEmbed as ASE\n",
    "from graspy.embed import MultipleASE as MASE\n",
    "from graspy.plot import pairplot\n",
    "from scipy.linalg import svdvals\n",
    "embedder = MASE()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first consider a series of stochastic block models (SBMs) consisting of 25 vertices per block.  We start with the simple two-block case.  The block probability matrix is given by\n",
    "$$P = \n",
    "\\begin{bmatrix}0.7 & 0.1\\\\\n",
    "0.1 & 0.7\n",
    "\\end{bmatrix}$$\n",
    "We will sample an SBM from these block probabilities, embed the sample graph with ASE, and visualize the result with a pairplot to show all the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [25, 25]\n",
    "P = [[0.7, 0.1],\n",
    "      [0.1, 0.7]]\n",
    "np.random.seed(8)\n",
    "G = sbm(n, P)\n",
    "embedder = ASE()\n",
    "X_hat = embedder.fit_transform(G)\n",
    "labels = [\"Block 1\"]*25 + [\"Block 2\"]*25\n",
    "cls = pairplot(X_hat, labels, title=\"Latent Positions of 2-Block SBM using ASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, ``AdjacecnySpectralEmbed()`` chooses to embed into 4 dimensions for a 2-block SBM, even though clearly only the first two contain meaningful information.  However, our algorithm is somewhat conservative and uses 2 elbows in the eigenvalues.  It isn't much of a problem if ``AdjacencySpectralEmbed()`` always gives a few extra dimensions, since we can always go back and prune them away.  Nevertheless, we would still expect some sort of linear relationship betwen the number of blocks $k$ and the chosen embedding dimension $d$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Embedding Dimension against Number of Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this, we vary $k$ from 2 to 100, and extend the probability matrix while maintaining the within-block and between-block probabilitites.  So, the 3-block block probability matrix is\n",
    "$$P = \n",
    "\\begin{bmatrix}0.7 & 0.1 & 0.1 \\\\\n",
    "               0.1 & 0.7 & 0.1 \\\\\n",
    "               0.1 & 0.1 & 0.7\n",
    "\\end{bmatrix}$$\n",
    "and so on.  We plot the embedding dimension $d$ against the number of blocks $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [25, 25]\n",
    "d = []\n",
    "k = []\n",
    "within_block = 0.7\n",
    "btwn_block = 0.1\n",
    "P = np.array([[within_block, btwn_block],\n",
    "      [btwn_block, within_block]])\n",
    "labels = [\"Block 1\"]*25 + [\"Block 2\"]*25\n",
    "\n",
    "\n",
    "for i in range(99):\n",
    "    #Sample sbm and embed\n",
    "    G = sbm(n, P)\n",
    "    X_hat = embedder.fit_transform(G)\n",
    "    \n",
    "    #Update variables \n",
    "    k.append(P.shape[0])\n",
    "    d.append(X_hat.shape[1])\n",
    "    n.append(25)\n",
    "    \n",
    "    #Extend P\n",
    "    concat1 = np.array([[btwn_block]*P.shape[0]])\n",
    "    concat2 = np.array([[btwn_block]]*P.shape[1]+[[within_block]])\n",
    "    P = np.concatenate((P, concat1), 0)\n",
    "    P = np.concatenate((P, concat2), 1)\n",
    "    \n",
    "d_vs_k = plt.scatter(k, d)\n",
    "plt.title(\"No relationship between the number of blocks and the chosen embedding dimension\")\n",
    "plt.xlabel(\"Number of Blocks\")\n",
    "plt.ylabel(\"Chosen embedding dimension\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the scatter plot above, we actually have basically no relationship at all (certainly not a linear one) between the number of blocks and the chosen embedding dimension, which would be problematic if we were relying solely on ``AdjacencySpectralEmbed()`` to choose the embedding dimension.  Luckily we have some other tools available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Singular Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``AdjacencySpectralEmbed()`` class uses an algorithm to estimate 'elbows' in a plot of singular values of the adjacency matrix, but we can also plot them directly ourselves.  Let's see the eigenvalues for our original example the two-block SBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [25, 25]\n",
    "P = [[0.7, 0.1],\n",
    "      [0.1, 0.7]]\n",
    "np.random.seed(8)\n",
    "G = sbm(n, P)\n",
    "embedder.set_params(algorithm=\"full\")\n",
    "X_hat = embedder.fit_transform(G)\n",
    "d = X_hat.shape[1]\n",
    "print(\"d=\", d)\n",
    "\n",
    "plt.figure()\n",
    "embedder_svds = plt.plot(embedder.singular_values_) #Bug -- why doesn't algorithm=full retain all the singular values???\n",
    "from scipy.linalg import svdvals\n",
    "plt.figure()\n",
    "G_svds = plt.plot(svdvals(G))\n",
    "plt.figure()\n",
    "G_svds_trunc = plt.plot(svdvals(G)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can clearly see that there is one distinct elbow that separates the first two singular values from the rest of the singular values, as expected with a 2-block SBM.  So, it seems like taking the first two elbows is giving too many dimensions in this case.  But, let's see what happens with a 100-block SBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100-Block SBM Singular Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the same extension of our 2-block matrix as before, keeping the within-block and between-block probabilites unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = 100\n",
    "n = [25]*k\n",
    "btwn_block = 0.01\n",
    "P = (within_block-btwn_block)*np.identity(k) + btwn_block*np.ones([k, k])\n",
    "G = sbm(n, P)\n",
    "embedder.set_params(algorithm=\"full\")\n",
    "X_hat = embedder.fit_transform(G)\n",
    "d = X_hat.shape[1]\n",
    "print(\"d=\", d) #Bug -- Why doesn't select_dimension correctly identify obvious elbows?\n",
    "\n",
    "plt.figure()\n",
    "embedder_svds = plt.plot(embedder.singular_values_) #Bug -- why doesn't algorithm=full retain all the singular values???\n",
    "plt.figure()\n",
    "G_svds = plt.plot(svdvals(G))\n",
    "plt.figure()\n",
    "G_svds_trunc = plt.plot(svdvals(G)[:2*k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in this case, there is one large elbow between the first two singular values, and the second elbow is between the 10th and 11th singular values (indicating the true dimensionality of the data).  So, by choosing ``n_elbows=2``, we account for this well-known phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a Little Variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why does such a large elbow occur between the first two singular values at all, when we know that the true dimensionality is the number of blocks, $k$?  We can get some insight to this by adding just a little bit of random variation to all of our numbers in the probability matrix so that they aren't so uniform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (within_block-btwn_block)*np.identity(k) + btwn_block*np.ones([k, k])\n",
    "from numpy.random import rand\n",
    "variation = np.triu(rand(P.shape[0], P.shape[1]))*0.01 #variation is upper triangular to maintain symmetry\n",
    "P += variation + np.transpose(variation)\n",
    "\n",
    "G = sbm(n, P)\n",
    "embedder.set_params(algorithm=\"full\")\n",
    "X_hat = embedder.fit_transform(G)\n",
    "d = X_hat.shape[1]\n",
    "print(\"d=\", d) #Bug -- Why doesn't select_dimension correctly identify obvious elbows?\n",
    "\n",
    "plt.figure()\n",
    "embedder_svds = plt.plot(embedder.singular_values_) #Bug -- why doesn't algorithm=full retain all the singular values???\n",
    "plt.figure()\n",
    "G_svds = plt.plot(svdvals(G))\n",
    "plt.figure()\n",
    "G_svds_trunc = plt.plot(svdvals(G)[:2*k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic Variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we systematically vary the amount of variation we inject into the system, and see how the relationship between the first eigenvalue and all the others changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [2**(-i) for i in range(3, 11)]\n",
    "signal = []\n",
    "for a in alpha:\n",
    "    P = (within_block-btwn_block)*np.identity(k) + btwn_block*np.ones([k, k])\n",
    "    variation = np.triu(rand(P.shape[0], P.shape[1]))*a #variation is upper triangular to maintain symmetry\n",
    "    P += variation + np.transpose(variation)\n",
    "    G = sbm(n, P)\n",
    "    embedder.set_params(algorithm=\"full\")\n",
    "    X_hat = embedder.fit_transform(G)\n",
    "    signal.append(np.mean(embedder.singular_values_[1:]) / embedder.singular_values_[0])\n",
    "    \n",
    "plt.figure()\n",
    "plt.xscale(\"log\")\n",
    "plt.plot(alpha, signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#debugging n_elbows\n",
    "n_blocks = 10\n",
    "P = np.identity(n_blocks)\n",
    "probs = np.linspace(0, 1, n_blocks)\n",
    "P[P==1] = probs\n",
    "plt.plot(svdvals(P))\n",
    "n = [50]*n_blocks\n",
    "G = np.array([sbm(n, P) for i in range(50)])\n",
    "G = np.mean(G, 0)\n",
    "plt.plot(svdvals(G)[0:20])\n",
    "#embedder = ASE()\n",
    "#V_hat = embedder.fit_transform(G)\n",
    "#plt.plot(embedder.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging n_elbows: does svd show repeated eigenvalues?\n",
    "P = np.identity(10)\n",
    "plt.plot(svdvals(P))\n",
    "#how do the eigenvalues look when sampled into an sbm?\n",
    "n = [25]*10\n",
    "G = sbm(n, P)\n",
    "embedder.fit(G)\n",
    "plt.plot(embedder.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging n_elbows\n",
    "#What if we steadily increase the between-block probabilites?\n",
    "btwn = np.linspace(0, 0.49, 10)\n",
    "for b in btwn:\n",
    "    plt.figure()\n",
    "    P = b*np.ones([10, 10])\n",
    "    probs = np.linspace(.5, 1-b, 10)\n",
    "    I = np.identity(10)\n",
    "    I[I==1] = probs\n",
    "    P += I\n",
    "    plt.plot(svdvals(P))\n",
    "    n = [25]*10\n",
    "    G = sbm(n, P)\n",
    "    embedder.fit(G)\n",
    "    plt.plot(svdvals(G)[0:20])\n",
    "    plt.plot(embedder.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging n_elbows\n",
    "#What if the between probabilities are not the same\n",
    "btwn_max = np.linspace(0.01, 0.49, 10)\n",
    "embedder.set_params(algorithm=\"full\")\n",
    "for b in btwn_max:\n",
    "    plt.figure()\n",
    "    btwn = np.linspace(b/(45), b, 45)   \n",
    "    P = np.identity(10)\n",
    "    P[P==0][:45] = btwn\n",
    "    P[P==0][45:] = np.flip(btwn)\n",
    "    plt.plot(svdvals(P))\n",
    "    n = [25]*10\n",
    "    G = sbm(n, P)\n",
    "    embedder.fit(G)\n",
    "    plt.plot(svdvals(G)[0:20])\n",
    "    plt.plot(embedder.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
